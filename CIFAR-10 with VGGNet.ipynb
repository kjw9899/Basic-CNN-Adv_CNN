{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exciting-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-rachel",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coupled-saturn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-rebel",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accessible-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polished-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self,model_code,in_channels):\n",
    "        super(VGG,self).__init__()\n",
    "        \n",
    "        self.layers=self._make_layers(model_code,in_channels)\n",
    "        self.classifer=nn.Sequential(nn.Linear(512,4096),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(4096,10))\n",
    "        \n",
    "    def forward (self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.classifer(x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _make_layers(self,model_code,in_channels):\n",
    "        layers=nn.ModuleList()\n",
    "        for x in cfg[model_code]:\n",
    "            if x != 'M':\n",
    "                layers.append(nn.Conv2d(in_channels,x,3,1,1))\n",
    "                layers.append(nn.ReLU())\n",
    "                in_channels=x\n",
    "            else:\n",
    "                layers.append(nn.MaxPool2d(2,2))\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "correct-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_check():\n",
    "    net=VGG('VGG19',3)\n",
    "    x=torch.randn(3,3,32,32)\n",
    "    y=net(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spoken-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "dimension_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-symposium",
   "metadata": {},
   "source": [
    "## Train,Validation,Test and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "white-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
    "                                              batch_size=args.train_batch_size, \n",
    "                                              shuffle=True, num_workers=2)\n",
    "    net.train()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dangerous-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                            batch_size=args.test_batch_size, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "composite-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "                                             batch_size=args.test_batch_size, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coral-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "  \n",
    "    net = VGG(args.model_code,args.in_channels)\n",
    "    net.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # ===================================== #\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # ============================ #\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc = test(net, partition, args)    \n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result\n",
    "    # ===================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decimal-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "    del setting['test_batch_size'] ## epoch, batch_size는 실험에 영향을 미치지 않는 변수이기 때문에 삭제\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting) ## result라는 dic에 setting도 합치기 위함\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "located-person",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=10, exp_name='exp1_model_code_lr', in_channels=3, l2=1e-05, lr=0.0015, model_code='VGG11', optim='Adam', test_batch_size=1024, train_batch_size=128)\n",
      "Epoch 0, Acc(train/val): 18.95/27.97, Loss(train/val) 2.00/1.81. Took 6.47 sec\n",
      "Epoch 1, Acc(train/val): 33.89/41.30, Loss(train/val) 1.66/1.50. Took 5.96 sec\n",
      "Epoch 2, Acc(train/val): 46.62/53.92, Loss(train/val) 1.41/1.28. Took 5.96 sec\n",
      "Epoch 3, Acc(train/val): 55.83/59.02, Loss(train/val) 1.21/1.14. Took 5.97 sec\n",
      "Epoch 4, Acc(train/val): 61.33/61.32, Loss(train/val) 1.08/1.09. Took 5.97 sec\n",
      "Epoch 5, Acc(train/val): 64.81/62.80, Loss(train/val) 0.99/1.06. Took 5.97 sec\n",
      "Epoch 6, Acc(train/val): 67.86/64.83, Loss(train/val) 0.91/1.00. Took 6.01 sec\n",
      "Epoch 7, Acc(train/val): 70.15/66.62, Loss(train/val) 0.84/0.98. Took 5.98 sec\n",
      "Epoch 8, Acc(train/val): 72.46/67.93, Loss(train/val) 0.79/0.94. Took 6.04 sec\n",
      "Epoch 9, Acc(train/val): 74.48/67.04, Loss(train/val) 0.73/0.98. Took 6.03 sec\n",
      "Namespace(epoch=10, exp_name='exp1_model_code_lr', in_channels=3, l2=1e-05, lr=0.001, model_code='VGG11', optim='Adam', test_batch_size=1024, train_batch_size=128)\n",
      "Epoch 0, Acc(train/val): 21.13/30.28, Loss(train/val) 1.96/1.77. Took 6.04 sec\n",
      "Epoch 1, Acc(train/val): 41.00/49.76, Loss(train/val) 1.53/1.33. Took 6.05 sec\n",
      "Epoch 2, Acc(train/val): 53.55/58.49, Loss(train/val) 1.25/1.15. Took 6.07 sec\n",
      "Epoch 3, Acc(train/val): 62.22/63.28, Loss(train/val) 1.05/1.02. Took 6.08 sec\n",
      "Epoch 4, Acc(train/val): 67.90/64.53, Loss(train/val) 0.90/1.03. Took 6.07 sec\n",
      "Epoch 5, Acc(train/val): 72.32/70.47, Loss(train/val) 0.79/0.84. Took 6.05 sec\n",
      "Epoch 6, Acc(train/val): 75.94/71.52, Loss(train/val) 0.68/0.83. Took 6.10 sec\n",
      "Epoch 7, Acc(train/val): 79.31/72.29, Loss(train/val) 0.59/0.84. Took 6.12 sec\n",
      "Epoch 8, Acc(train/val): 82.04/72.87, Loss(train/val) 0.52/0.86. Took 6.11 sec\n",
      "Epoch 9, Acc(train/val): 84.35/73.32, Loss(train/val) 0.45/0.83. Took 6.08 sec\n",
      "Namespace(epoch=10, exp_name='exp1_model_code_lr', in_channels=3, l2=1e-05, lr=0.0015, model_code='VGG13', optim='Adam', test_batch_size=1024, train_batch_size=128)\n",
      "Epoch 0, Acc(train/val): 21.55/28.08, Loss(train/val) 1.98/1.82. Took 8.56 sec\n",
      "Epoch 1, Acc(train/val): 33.63/38.66, Loss(train/val) 1.70/1.57. Took 8.52 sec\n",
      "Epoch 2, Acc(train/val): 45.75/51.35, Loss(train/val) 1.44/1.32. Took 8.53 sec\n",
      "Epoch 3, Acc(train/val): 56.31/58.80, Loss(train/val) 1.21/1.14. Took 8.54 sec\n",
      "Epoch 4, Acc(train/val): 62.54/63.50, Loss(train/val) 1.04/1.02. Took 8.55 sec\n",
      "Epoch 5, Acc(train/val): 67.36/65.62, Loss(train/val) 0.92/0.98. Took 8.54 sec\n",
      "Epoch 6, Acc(train/val): 70.59/68.22, Loss(train/val) 0.83/0.92. Took 8.50 sec\n",
      "Epoch 7, Acc(train/val): 73.73/69.27, Loss(train/val) 0.75/0.90. Took 8.54 sec\n",
      "Epoch 8, Acc(train/val): 76.05/71.60, Loss(train/val) 0.68/0.84. Took 8.58 sec\n",
      "Epoch 9, Acc(train/val): 78.75/71.91, Loss(train/val) 0.61/0.83. Took 8.57 sec\n",
      "Namespace(epoch=10, exp_name='exp1_model_code_lr', in_channels=3, l2=1e-05, lr=0.001, model_code='VGG13', optim='Adam', test_batch_size=1024, train_batch_size=128)\n",
      "Epoch 0, Acc(train/val): 23.09/30.50, Loss(train/val) 1.93/1.74. Took 8.59 sec\n",
      "Epoch 1, Acc(train/val): 40.02/49.42, Loss(train/val) 1.54/1.36. Took 8.58 sec\n",
      "Epoch 2, Acc(train/val): 54.35/60.45, Loss(train/val) 1.24/1.11. Took 8.58 sec\n",
      "Epoch 3, Acc(train/val): 64.05/65.69, Loss(train/val) 1.00/1.00. Took 8.61 sec\n",
      "Epoch 4, Acc(train/val): 70.43/70.35, Loss(train/val) 0.84/0.85. Took 8.60 sec\n",
      "Epoch 5, Acc(train/val): 74.91/71.83, Loss(train/val) 0.72/0.82. Took 8.60 sec\n",
      "Epoch 6, Acc(train/val): 77.86/74.00, Loss(train/val) 0.64/0.77. Took 8.60 sec\n",
      "Epoch 7, Acc(train/val): 81.39/75.01, Loss(train/val) 0.55/0.77. Took 8.58 sec\n",
      "Epoch 8, Acc(train/val): 84.00/75.96, Loss(train/val) 0.47/0.74. Took 8.63 sec\n",
      "Epoch 9, Acc(train/val): 86.13/75.15, Loss(train/val) 0.40/0.83. Took 8.56 sec\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_model_code_lr\"\n",
    "\n",
    "# ====== Model Capacity ====== #\n",
    "args.in_channels=3\n",
    "args.model_code='VGG11'\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.00001\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0015\n",
    "args.epoch = 10\n",
    "\n",
    "args.train_batch_size = 128\n",
    "args.test_batch_size = 1024\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'model_code'\n",
    "name_var2 = 'lr'\n",
    "list_var1 = ['VGG11','VGG13']\n",
    "list_var2 = [0.0015, 0.001]\n",
    "\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1) ## args.name_var1 = var1 과 같은 말\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        setting, result = experiment(partition, deepcopy(args))\n",
    "        save_exp_result(setting, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-pressure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
